# Since the performance of the model isn't satisfactory, we'll try Decision Tree Regressor and Random Forest Regression models.
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

# Train and evaluate Decision Tree Regressor
tree_model = DecisionTreeRegressor(random_state=42)
tree_model.fit(X_train, y_train)
y_pred_tree = tree_model.predict(X_test)
evaluate_model(y_test, y_pred_tree, "Decision Tree Regressor")

# Train and evaluate Random Forest Regressor
forest_model = RandomForestRegressor(random_state=42)
forest_model.fit(X_train, y_train)
y_pred_forest = forest_model.predict(X_test)
evaluate_model(y_test, y_pred_forest, "Random Forest Regressor")
Decision Tree Regressor Metrics:
  - Mean Squared Error (MSE): 5170.949298088411
  - Mean Absolute Error (MAE): 43.36544205495819
  - R² Score: 0.7750311064647104

Random Forest Regressor Metrics:
  - Mean Squared Error (MSE): 2664.211293835009
  - Mean Absolute Error (MAE): 32.660611011549186
  - R² Score: 0.8840900128067671

The Random Forest Regressor seems to be the best-performing model for this dataset, achieving the lowest error (MSE = 2690.36, MAE = 30.89) and the highest explanatory power (R² = 0.88).

# creating bar plots to visually compare the performance of the Linear Regression, Decision Tree Regressor, and Random Forest Regressor models

# Metrics for each model
models = ['Linear Regression', 'Decision Tree', 'Random Forest']
mse_scores = [7074.59, 4722.82, 2527.64]
mae_scores = [62.64, 39.54, 30.35]
r2_scores = [0.67, 0.78, 0.88]

# Set up the figure and axes
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Plot MSE
axes[0].bar(models, mse_scores, color=['palevioletred', 'lightskyblue', 'lightsalmon'])
axes[0].set_title('Mean Squared Error (MSE)')
axes[0].set_ylabel('MSE')
axes[0].set_ylim(0, max(mse_scores) + 1000)  # Adjust y-axis limit for better visualization

# Plot MAE
axes[1].bar(models, mae_scores, color=['palevioletred', 'lightskyblue', 'lightsalmon'])
axes[1].set_title('Mean Absolute Error (MAE)')
axes[1].set_ylabel('MAE')
axes[1].set_ylim(0, max(mae_scores) + 10)  # Adjust y-axis limit for better visualization

# Plot R² Score
axes[2].bar(models, r2_scores, color=['palevioletred', 'lightskyblue', 'lightsalmon'])
axes[2].set_title('R² Score')
axes[2].set_ylabel('R² Score')
axes[2].set_ylim(0, 1)  # R² Score ranges from 0 to 1

# Add value labels on top of each bar
for i, ax in enumerate(axes):
    for j, v in enumerate([mse_scores, mae_scores, r2_scores][i]):
        ax.text(j, v + 0.02 * max([mse_scores, mae_scores, r2_scores][i]), 
                f'{v:.2f}', ha='center', va='bottom')

# Adjust layout and display
plt.tight_layout()
plt.show()

# Residual analysis:  Identifying patterns in the errors to improve the model.
import matplotlib.pyplot as plt

# Calculate residuals
residuals = y_test - y_pred_forest

# Plot residuals
plt.figure(figsize=(10, 6))
plt.scatter(y_pred_forest, residuals, alpha=0.5)
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()

The residual plot indicates that the model has room for improvement, particularly in handling higher bike rental counts and reducing heteroscedasticity. By hyperparameter tuning, and cross-validation, we can build a more robust and accurate model.
